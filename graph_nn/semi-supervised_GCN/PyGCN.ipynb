{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyGCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgj1DgspkaX2",
        "colab_type": "text"
      },
      "source": [
        "### reference following this [paper](https://arxiv.org/abs/1609.02907)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1irP9ZoVja1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tkipf/pygcn.git   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJZqhGcEVtb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/pygcn/pygcn\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOEBwvNYVPOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "from utils import *"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0E_ItpybtxD",
        "colab_type": "text"
      },
      "source": [
        "### Layer definition\n",
        "\n",
        "we define a weight matrix $W$ with dimensions (input_dimension, output_dimension) \n",
        "\n",
        "Example if the feature vector of the input layer has D = 10 and i want the output to be D = 15, the matrix will be of shape (10 x 15)\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rS_P85tUdj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    \n",
        "    # This is a function that does Xavier initalization.\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight) # matrix multiplication of the input with the weight matrix\n",
        "        output = torch.spmm(adj, support) # then sparse matrix multiplication with the normalized laplacian matrix as in the paper\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMK5hIJ-er-J",
        "colab_type": "text"
      },
      "source": [
        "### Building the model\n",
        "- The model is simply 2 graph convolutional layers\n",
        "- After the first grah convolutinal layer we apply a ReLU function to introduce the non-linearity.\n",
        "- After the second layer we pass it through a softmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoir2z3pUjyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-jkK3zf9U-",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Description   (Cora dataset)\n",
        "\n",
        "\n",
        "\n",
        "The Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:\n",
        "\t\tCase_Based\n",
        "\t\tGenetic_Algorithms\n",
        "\t\tNeural_Networks\n",
        "\t\tProbabilistic_Methods\n",
        "\t\tReinforcement_Learning\n",
        "\t\tRule_Learning\n",
        "\t\tTheory\n",
        "\n",
        "\n",
        "THE DIRECTORY CONTAINS TWO FILES:\n",
        "\n",
        "The .content file contains descriptions of the papers in the following format:\n",
        "\n",
        "\t\t<paper_id> <word_attributes>+ <class_label>\n",
        "\n",
        "\n",
        "The .cites file contains the citation graph of the corpus. Each line describes a link in the following format:\n",
        "\n",
        "\t\t<ID of cited paper> <ID of citing paper>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Fr9Pakggzg",
        "colab_type": "text"
      },
      "source": [
        "### Main file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaek2JKMZqOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from pygcn.utils import load_data, accuracy\n",
        "from pygcn.models import GCN\n",
        "\n",
        "\n",
        "# This is just a parser for optional hyper-parameters\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')                                 # CUDA\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')                      \n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')            # Random seed\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')                              # num of epochs\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')                                  # learning rate\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')                   # Decay rate for adam optimization\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')                                 # hidden layer dimension\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()   \n",
        "\n",
        "\n",
        "# observe\n",
        "# idx_train = range(140)\n",
        "# idx_val = range(200, 500)\n",
        "# idx_test = range(500, 1500)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])   # cross entropy loss\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJdnNQbyadE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d2eab21-80bb-4c9d-c8cb-28d056a6d4be"
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9342 acc_train: 0.1286 loss_val: 1.9261 acc_val: 0.1567 time: 0.1707s\n",
            "Epoch: 0002 loss_train: 1.9133 acc_train: 0.2214 loss_val: 1.9156 acc_val: 0.1567 time: 0.0189s\n",
            "Epoch: 0003 loss_train: 1.9170 acc_train: 0.2143 loss_val: 1.9055 acc_val: 0.1567 time: 0.0186s\n",
            "Epoch: 0004 loss_train: 1.9008 acc_train: 0.2214 loss_val: 1.8961 acc_val: 0.1600 time: 0.0187s\n",
            "Epoch: 0005 loss_train: 1.8936 acc_train: 0.2214 loss_val: 1.8868 acc_val: 0.1767 time: 0.0193s\n",
            "Epoch: 0006 loss_train: 1.8725 acc_train: 0.3571 loss_val: 1.8778 acc_val: 0.3500 time: 0.0187s\n",
            "Epoch: 0007 loss_train: 1.8775 acc_train: 0.3429 loss_val: 1.8692 acc_val: 0.4433 time: 0.0188s\n",
            "Epoch: 0008 loss_train: 1.8682 acc_train: 0.3786 loss_val: 1.8610 acc_val: 0.4433 time: 0.0187s\n",
            "Epoch: 0009 loss_train: 1.8590 acc_train: 0.4000 loss_val: 1.8528 acc_val: 0.3900 time: 0.0183s\n",
            "Epoch: 0010 loss_train: 1.8485 acc_train: 0.3786 loss_val: 1.8445 acc_val: 0.3633 time: 0.0186s\n",
            "Epoch: 0011 loss_train: 1.8349 acc_train: 0.3643 loss_val: 1.8362 acc_val: 0.3600 time: 0.0189s\n",
            "Epoch: 0012 loss_train: 1.8403 acc_train: 0.3286 loss_val: 1.8281 acc_val: 0.3533 time: 0.0189s\n",
            "Epoch: 0013 loss_train: 1.8258 acc_train: 0.3357 loss_val: 1.8201 acc_val: 0.3533 time: 0.0196s\n",
            "Epoch: 0014 loss_train: 1.8063 acc_train: 0.3643 loss_val: 1.8122 acc_val: 0.3533 time: 0.0201s\n",
            "Epoch: 0015 loss_train: 1.8093 acc_train: 0.3500 loss_val: 1.8044 acc_val: 0.3533 time: 0.0208s\n",
            "Epoch: 0016 loss_train: 1.8045 acc_train: 0.3500 loss_val: 1.7967 acc_val: 0.3533 time: 0.0255s\n",
            "Epoch: 0017 loss_train: 1.7879 acc_train: 0.3357 loss_val: 1.7890 acc_val: 0.3533 time: 0.0199s\n",
            "Epoch: 0018 loss_train: 1.7898 acc_train: 0.3071 loss_val: 1.7816 acc_val: 0.3533 time: 0.0194s\n",
            "Epoch: 0019 loss_train: 1.7877 acc_train: 0.3143 loss_val: 1.7746 acc_val: 0.3533 time: 0.0188s\n",
            "Epoch: 0020 loss_train: 1.7692 acc_train: 0.3357 loss_val: 1.7678 acc_val: 0.3533 time: 0.0191s\n",
            "Epoch: 0021 loss_train: 1.7619 acc_train: 0.3286 loss_val: 1.7609 acc_val: 0.3533 time: 0.0182s\n",
            "Epoch: 0022 loss_train: 1.7403 acc_train: 0.3714 loss_val: 1.7539 acc_val: 0.3567 time: 0.0196s\n",
            "Epoch: 0023 loss_train: 1.7416 acc_train: 0.3571 loss_val: 1.7467 acc_val: 0.3600 time: 0.0199s\n",
            "Epoch: 0024 loss_train: 1.7175 acc_train: 0.3500 loss_val: 1.7397 acc_val: 0.3600 time: 0.0195s\n",
            "Epoch: 0025 loss_train: 1.7328 acc_train: 0.3500 loss_val: 1.7327 acc_val: 0.3633 time: 0.0196s\n",
            "Epoch: 0026 loss_train: 1.6941 acc_train: 0.3857 loss_val: 1.7255 acc_val: 0.3633 time: 0.0190s\n",
            "Epoch: 0027 loss_train: 1.6996 acc_train: 0.3786 loss_val: 1.7182 acc_val: 0.3633 time: 0.0192s\n",
            "Epoch: 0028 loss_train: 1.6867 acc_train: 0.4143 loss_val: 1.7109 acc_val: 0.3633 time: 0.0196s\n",
            "Epoch: 0029 loss_train: 1.6715 acc_train: 0.3714 loss_val: 1.7036 acc_val: 0.3633 time: 0.0194s\n",
            "Epoch: 0030 loss_train: 1.6952 acc_train: 0.3714 loss_val: 1.6963 acc_val: 0.3633 time: 0.0189s\n",
            "Epoch: 0031 loss_train: 1.6586 acc_train: 0.3857 loss_val: 1.6889 acc_val: 0.3700 time: 0.0232s\n",
            "Epoch: 0032 loss_train: 1.6628 acc_train: 0.3857 loss_val: 1.6813 acc_val: 0.3700 time: 0.0210s\n",
            "Epoch: 0033 loss_train: 1.6443 acc_train: 0.4000 loss_val: 1.6733 acc_val: 0.3700 time: 0.0219s\n",
            "Epoch: 0034 loss_train: 1.6133 acc_train: 0.4071 loss_val: 1.6650 acc_val: 0.3700 time: 0.0191s\n",
            "Epoch: 0035 loss_train: 1.6021 acc_train: 0.4143 loss_val: 1.6565 acc_val: 0.3700 time: 0.0204s\n",
            "Epoch: 0036 loss_train: 1.6431 acc_train: 0.3857 loss_val: 1.6476 acc_val: 0.3733 time: 0.0188s\n",
            "Epoch: 0037 loss_train: 1.5996 acc_train: 0.4357 loss_val: 1.6386 acc_val: 0.3733 time: 0.0205s\n",
            "Epoch: 0038 loss_train: 1.5780 acc_train: 0.4143 loss_val: 1.6293 acc_val: 0.3767 time: 0.0202s\n",
            "Epoch: 0039 loss_train: 1.5817 acc_train: 0.3643 loss_val: 1.6197 acc_val: 0.3867 time: 0.0252s\n",
            "Epoch: 0040 loss_train: 1.5734 acc_train: 0.4143 loss_val: 1.6099 acc_val: 0.4000 time: 0.0280s\n",
            "Epoch: 0041 loss_train: 1.5494 acc_train: 0.4357 loss_val: 1.5994 acc_val: 0.4100 time: 0.0188s\n",
            "Epoch: 0042 loss_train: 1.5328 acc_train: 0.4429 loss_val: 1.5887 acc_val: 0.4100 time: 0.0188s\n",
            "Epoch: 0043 loss_train: 1.5022 acc_train: 0.4286 loss_val: 1.5775 acc_val: 0.4100 time: 0.0184s\n",
            "Epoch: 0044 loss_train: 1.4958 acc_train: 0.4571 loss_val: 1.5659 acc_val: 0.4100 time: 0.0199s\n",
            "Epoch: 0045 loss_train: 1.4827 acc_train: 0.4571 loss_val: 1.5541 acc_val: 0.4167 time: 0.0245s\n",
            "Epoch: 0046 loss_train: 1.4581 acc_train: 0.4786 loss_val: 1.5419 acc_val: 0.4267 time: 0.0237s\n",
            "Epoch: 0047 loss_train: 1.4574 acc_train: 0.4500 loss_val: 1.5291 acc_val: 0.4367 time: 0.0208s\n",
            "Epoch: 0048 loss_train: 1.4369 acc_train: 0.4643 loss_val: 1.5162 acc_val: 0.4400 time: 0.0205s\n",
            "Epoch: 0049 loss_train: 1.3986 acc_train: 0.4786 loss_val: 1.5028 acc_val: 0.4500 time: 0.0216s\n",
            "Epoch: 0050 loss_train: 1.3888 acc_train: 0.5000 loss_val: 1.4889 acc_val: 0.4533 time: 0.0207s\n",
            "Epoch: 0051 loss_train: 1.3550 acc_train: 0.4929 loss_val: 1.4749 acc_val: 0.4633 time: 0.0194s\n",
            "Epoch: 0052 loss_train: 1.3472 acc_train: 0.4857 loss_val: 1.4612 acc_val: 0.4700 time: 0.0187s\n",
            "Epoch: 0053 loss_train: 1.3648 acc_train: 0.5357 loss_val: 1.4473 acc_val: 0.4967 time: 0.0187s\n",
            "Epoch: 0054 loss_train: 1.3084 acc_train: 0.5500 loss_val: 1.4329 acc_val: 0.5000 time: 0.0195s\n",
            "Epoch: 0055 loss_train: 1.3026 acc_train: 0.5786 loss_val: 1.4182 acc_val: 0.5067 time: 0.0218s\n",
            "Epoch: 0056 loss_train: 1.2970 acc_train: 0.5714 loss_val: 1.4032 acc_val: 0.5333 time: 0.0197s\n",
            "Epoch: 0057 loss_train: 1.2770 acc_train: 0.5929 loss_val: 1.3885 acc_val: 0.5567 time: 0.0195s\n",
            "Epoch: 0058 loss_train: 1.2443 acc_train: 0.5929 loss_val: 1.3740 acc_val: 0.5767 time: 0.0206s\n",
            "Epoch: 0059 loss_train: 1.2502 acc_train: 0.6357 loss_val: 1.3597 acc_val: 0.5867 time: 0.0209s\n",
            "Epoch: 0060 loss_train: 1.2289 acc_train: 0.6357 loss_val: 1.3455 acc_val: 0.6033 time: 0.0202s\n",
            "Epoch: 0061 loss_train: 1.2422 acc_train: 0.6000 loss_val: 1.3313 acc_val: 0.6200 time: 0.0190s\n",
            "Epoch: 0062 loss_train: 1.1853 acc_train: 0.6286 loss_val: 1.3176 acc_val: 0.6300 time: 0.0201s\n",
            "Epoch: 0063 loss_train: 1.1384 acc_train: 0.6929 loss_val: 1.3039 acc_val: 0.6500 time: 0.0199s\n",
            "Epoch: 0064 loss_train: 1.1401 acc_train: 0.6714 loss_val: 1.2903 acc_val: 0.6767 time: 0.0197s\n",
            "Epoch: 0065 loss_train: 1.1430 acc_train: 0.6643 loss_val: 1.2764 acc_val: 0.6900 time: 0.0215s\n",
            "Epoch: 0066 loss_train: 1.1029 acc_train: 0.7286 loss_val: 1.2627 acc_val: 0.6933 time: 0.0189s\n",
            "Epoch: 0067 loss_train: 1.0864 acc_train: 0.7429 loss_val: 1.2491 acc_val: 0.6933 time: 0.0190s\n",
            "Epoch: 0068 loss_train: 1.0974 acc_train: 0.7643 loss_val: 1.2359 acc_val: 0.7033 time: 0.0195s\n",
            "Epoch: 0069 loss_train: 1.0737 acc_train: 0.7500 loss_val: 1.2232 acc_val: 0.7033 time: 0.0194s\n",
            "Epoch: 0070 loss_train: 1.0653 acc_train: 0.7429 loss_val: 1.2112 acc_val: 0.7100 time: 0.0190s\n",
            "Epoch: 0071 loss_train: 1.0165 acc_train: 0.7929 loss_val: 1.1999 acc_val: 0.7167 time: 0.0203s\n",
            "Epoch: 0072 loss_train: 1.0413 acc_train: 0.7429 loss_val: 1.1894 acc_val: 0.7300 time: 0.0196s\n",
            "Epoch: 0073 loss_train: 1.0421 acc_train: 0.7071 loss_val: 1.1792 acc_val: 0.7367 time: 0.0193s\n",
            "Epoch: 0074 loss_train: 1.0142 acc_train: 0.7643 loss_val: 1.1694 acc_val: 0.7467 time: 0.0205s\n",
            "Epoch: 0075 loss_train: 0.9779 acc_train: 0.7857 loss_val: 1.1598 acc_val: 0.7533 time: 0.0216s\n",
            "Epoch: 0076 loss_train: 1.0080 acc_train: 0.7857 loss_val: 1.1489 acc_val: 0.7567 time: 0.0186s\n",
            "Epoch: 0077 loss_train: 0.9619 acc_train: 0.8000 loss_val: 1.1376 acc_val: 0.7600 time: 0.0191s\n",
            "Epoch: 0078 loss_train: 0.9607 acc_train: 0.8071 loss_val: 1.1265 acc_val: 0.7600 time: 0.0193s\n",
            "Epoch: 0079 loss_train: 0.9548 acc_train: 0.7643 loss_val: 1.1165 acc_val: 0.7600 time: 0.0190s\n",
            "Epoch: 0080 loss_train: 0.9305 acc_train: 0.8000 loss_val: 1.1068 acc_val: 0.7633 time: 0.0191s\n",
            "Epoch: 0081 loss_train: 0.9128 acc_train: 0.8071 loss_val: 1.0972 acc_val: 0.7633 time: 0.0184s\n",
            "Epoch: 0082 loss_train: 0.8937 acc_train: 0.7714 loss_val: 1.0885 acc_val: 0.7667 time: 0.0207s\n",
            "Epoch: 0083 loss_train: 0.9348 acc_train: 0.8214 loss_val: 1.0802 acc_val: 0.7767 time: 0.0229s\n",
            "Epoch: 0084 loss_train: 0.8909 acc_train: 0.8143 loss_val: 1.0725 acc_val: 0.7767 time: 0.0198s\n",
            "Epoch: 0085 loss_train: 0.8607 acc_train: 0.8071 loss_val: 1.0652 acc_val: 0.7833 time: 0.0213s\n",
            "Epoch: 0086 loss_train: 0.8969 acc_train: 0.7929 loss_val: 1.0577 acc_val: 0.7867 time: 0.0198s\n",
            "Epoch: 0087 loss_train: 0.9093 acc_train: 0.8143 loss_val: 1.0483 acc_val: 0.7833 time: 0.0199s\n",
            "Epoch: 0088 loss_train: 0.8888 acc_train: 0.8000 loss_val: 1.0388 acc_val: 0.7800 time: 0.0193s\n",
            "Epoch: 0089 loss_train: 0.8650 acc_train: 0.8071 loss_val: 1.0296 acc_val: 0.7900 time: 0.0190s\n",
            "Epoch: 0090 loss_train: 0.8372 acc_train: 0.8429 loss_val: 1.0212 acc_val: 0.7933 time: 0.0204s\n",
            "Epoch: 0091 loss_train: 0.8398 acc_train: 0.8214 loss_val: 1.0137 acc_val: 0.7933 time: 0.0190s\n",
            "Epoch: 0092 loss_train: 0.7930 acc_train: 0.8214 loss_val: 1.0071 acc_val: 0.7900 time: 0.0186s\n",
            "Epoch: 0093 loss_train: 0.8284 acc_train: 0.8071 loss_val: 1.0010 acc_val: 0.7867 time: 0.0189s\n",
            "Epoch: 0094 loss_train: 0.8234 acc_train: 0.8714 loss_val: 0.9950 acc_val: 0.7800 time: 0.0186s\n",
            "Epoch: 0095 loss_train: 0.8407 acc_train: 0.8643 loss_val: 0.9889 acc_val: 0.7800 time: 0.0201s\n",
            "Epoch: 0096 loss_train: 0.8456 acc_train: 0.8214 loss_val: 0.9821 acc_val: 0.7833 time: 0.0189s\n",
            "Epoch: 0097 loss_train: 0.7703 acc_train: 0.8643 loss_val: 0.9747 acc_val: 0.7800 time: 0.0188s\n",
            "Epoch: 0098 loss_train: 0.7575 acc_train: 0.8643 loss_val: 0.9672 acc_val: 0.7867 time: 0.0188s\n",
            "Epoch: 0099 loss_train: 0.7588 acc_train: 0.8429 loss_val: 0.9606 acc_val: 0.7933 time: 0.0192s\n",
            "Epoch: 0100 loss_train: 0.7566 acc_train: 0.8214 loss_val: 0.9541 acc_val: 0.8033 time: 0.0197s\n",
            "Epoch: 0101 loss_train: 0.7425 acc_train: 0.8429 loss_val: 0.9480 acc_val: 0.8067 time: 0.0193s\n",
            "Epoch: 0102 loss_train: 0.7279 acc_train: 0.8500 loss_val: 0.9423 acc_val: 0.8067 time: 0.0193s\n",
            "Epoch: 0103 loss_train: 0.7112 acc_train: 0.8571 loss_val: 0.9375 acc_val: 0.7933 time: 0.0194s\n",
            "Epoch: 0104 loss_train: 0.6963 acc_train: 0.8571 loss_val: 0.9323 acc_val: 0.7933 time: 0.0197s\n",
            "Epoch: 0105 loss_train: 0.7032 acc_train: 0.8643 loss_val: 0.9278 acc_val: 0.7900 time: 0.0201s\n",
            "Epoch: 0106 loss_train: 0.6961 acc_train: 0.8786 loss_val: 0.9239 acc_val: 0.7933 time: 0.0214s\n",
            "Epoch: 0107 loss_train: 0.7325 acc_train: 0.8643 loss_val: 0.9196 acc_val: 0.7933 time: 0.0212s\n",
            "Epoch: 0108 loss_train: 0.7077 acc_train: 0.8857 loss_val: 0.9147 acc_val: 0.7933 time: 0.0197s\n",
            "Epoch: 0109 loss_train: 0.7063 acc_train: 0.8571 loss_val: 0.9093 acc_val: 0.7900 time: 0.0194s\n",
            "Epoch: 0110 loss_train: 0.7063 acc_train: 0.8500 loss_val: 0.9042 acc_val: 0.7967 time: 0.0214s\n",
            "Epoch: 0111 loss_train: 0.6792 acc_train: 0.8714 loss_val: 0.8991 acc_val: 0.8000 time: 0.0198s\n",
            "Epoch: 0112 loss_train: 0.6642 acc_train: 0.8714 loss_val: 0.8946 acc_val: 0.8000 time: 0.0194s\n",
            "Epoch: 0113 loss_train: 0.6897 acc_train: 0.8500 loss_val: 0.8899 acc_val: 0.8033 time: 0.0202s\n",
            "Epoch: 0114 loss_train: 0.6811 acc_train: 0.8357 loss_val: 0.8855 acc_val: 0.8033 time: 0.0203s\n",
            "Epoch: 0115 loss_train: 0.6855 acc_train: 0.8786 loss_val: 0.8811 acc_val: 0.8033 time: 0.0201s\n",
            "Epoch: 0116 loss_train: 0.6880 acc_train: 0.8643 loss_val: 0.8787 acc_val: 0.7933 time: 0.0193s\n",
            "Epoch: 0117 loss_train: 0.6392 acc_train: 0.8786 loss_val: 0.8764 acc_val: 0.7900 time: 0.0187s\n",
            "Epoch: 0118 loss_train: 0.6282 acc_train: 0.9143 loss_val: 0.8720 acc_val: 0.7867 time: 0.0191s\n",
            "Epoch: 0119 loss_train: 0.6504 acc_train: 0.8643 loss_val: 0.8673 acc_val: 0.7867 time: 0.0190s\n",
            "Epoch: 0120 loss_train: 0.6231 acc_train: 0.8929 loss_val: 0.8615 acc_val: 0.8000 time: 0.0196s\n",
            "Epoch: 0121 loss_train: 0.6658 acc_train: 0.8500 loss_val: 0.8549 acc_val: 0.8100 time: 0.0224s\n",
            "Epoch: 0122 loss_train: 0.6102 acc_train: 0.8571 loss_val: 0.8500 acc_val: 0.8033 time: 0.0203s\n",
            "Epoch: 0123 loss_train: 0.5840 acc_train: 0.9143 loss_val: 0.8455 acc_val: 0.8033 time: 0.0189s\n",
            "Epoch: 0124 loss_train: 0.6686 acc_train: 0.8286 loss_val: 0.8423 acc_val: 0.8067 time: 0.0205s\n",
            "Epoch: 0125 loss_train: 0.6037 acc_train: 0.8857 loss_val: 0.8391 acc_val: 0.8067 time: 0.0213s\n",
            "Epoch: 0126 loss_train: 0.5768 acc_train: 0.8714 loss_val: 0.8358 acc_val: 0.8133 time: 0.0210s\n",
            "Epoch: 0127 loss_train: 0.6179 acc_train: 0.8786 loss_val: 0.8330 acc_val: 0.8033 time: 0.0200s\n",
            "Epoch: 0128 loss_train: 0.6274 acc_train: 0.8643 loss_val: 0.8292 acc_val: 0.8033 time: 0.0194s\n",
            "Epoch: 0129 loss_train: 0.5824 acc_train: 0.8929 loss_val: 0.8262 acc_val: 0.8000 time: 0.0196s\n",
            "Epoch: 0130 loss_train: 0.5831 acc_train: 0.9071 loss_val: 0.8239 acc_val: 0.7967 time: 0.0192s\n",
            "Epoch: 0131 loss_train: 0.5961 acc_train: 0.9071 loss_val: 0.8221 acc_val: 0.7900 time: 0.0192s\n",
            "Epoch: 0132 loss_train: 0.5738 acc_train: 0.9071 loss_val: 0.8182 acc_val: 0.7933 time: 0.0190s\n",
            "Epoch: 0133 loss_train: 0.5271 acc_train: 0.9071 loss_val: 0.8135 acc_val: 0.8033 time: 0.0213s\n",
            "Epoch: 0134 loss_train: 0.5362 acc_train: 0.8857 loss_val: 0.8086 acc_val: 0.8100 time: 0.0230s\n",
            "Epoch: 0135 loss_train: 0.5371 acc_train: 0.9214 loss_val: 0.8046 acc_val: 0.8200 time: 0.0203s\n",
            "Epoch: 0136 loss_train: 0.5664 acc_train: 0.9071 loss_val: 0.8015 acc_val: 0.8200 time: 0.0233s\n",
            "Epoch: 0137 loss_train: 0.5563 acc_train: 0.9000 loss_val: 0.7992 acc_val: 0.8100 time: 0.0204s\n",
            "Epoch: 0138 loss_train: 0.5593 acc_train: 0.9071 loss_val: 0.7972 acc_val: 0.8067 time: 0.0201s\n",
            "Epoch: 0139 loss_train: 0.5249 acc_train: 0.9143 loss_val: 0.7954 acc_val: 0.8033 time: 0.0187s\n",
            "Epoch: 0140 loss_train: 0.5439 acc_train: 0.9214 loss_val: 0.7930 acc_val: 0.8000 time: 0.0188s\n",
            "Epoch: 0141 loss_train: 0.5503 acc_train: 0.8786 loss_val: 0.7905 acc_val: 0.7967 time: 0.0187s\n",
            "Epoch: 0142 loss_train: 0.5272 acc_train: 0.8786 loss_val: 0.7868 acc_val: 0.7967 time: 0.0193s\n",
            "Epoch: 0143 loss_train: 0.5426 acc_train: 0.8929 loss_val: 0.7829 acc_val: 0.8000 time: 0.0206s\n",
            "Epoch: 0144 loss_train: 0.5254 acc_train: 0.9000 loss_val: 0.7796 acc_val: 0.8067 time: 0.0187s\n",
            "Epoch: 0145 loss_train: 0.5185 acc_train: 0.9071 loss_val: 0.7776 acc_val: 0.8000 time: 0.0188s\n",
            "Epoch: 0146 loss_train: 0.5305 acc_train: 0.8786 loss_val: 0.7758 acc_val: 0.8000 time: 0.0192s\n",
            "Epoch: 0147 loss_train: 0.5154 acc_train: 0.9143 loss_val: 0.7740 acc_val: 0.8000 time: 0.0191s\n",
            "Epoch: 0148 loss_train: 0.5064 acc_train: 0.8857 loss_val: 0.7718 acc_val: 0.8000 time: 0.0198s\n",
            "Epoch: 0149 loss_train: 0.5163 acc_train: 0.9214 loss_val: 0.7698 acc_val: 0.8000 time: 0.0202s\n",
            "Epoch: 0150 loss_train: 0.4932 acc_train: 0.9000 loss_val: 0.7679 acc_val: 0.8033 time: 0.0208s\n",
            "Epoch: 0151 loss_train: 0.4857 acc_train: 0.9357 loss_val: 0.7654 acc_val: 0.8033 time: 0.0234s\n",
            "Epoch: 0152 loss_train: 0.5050 acc_train: 0.8929 loss_val: 0.7631 acc_val: 0.8067 time: 0.0194s\n",
            "Epoch: 0153 loss_train: 0.5075 acc_train: 0.9071 loss_val: 0.7613 acc_val: 0.8100 time: 0.0193s\n",
            "Epoch: 0154 loss_train: 0.5250 acc_train: 0.9071 loss_val: 0.7590 acc_val: 0.8133 time: 0.0197s\n",
            "Epoch: 0155 loss_train: 0.5157 acc_train: 0.9143 loss_val: 0.7564 acc_val: 0.8167 time: 0.0192s\n",
            "Epoch: 0156 loss_train: 0.5005 acc_train: 0.8929 loss_val: 0.7553 acc_val: 0.8100 time: 0.0217s\n",
            "Epoch: 0157 loss_train: 0.4909 acc_train: 0.9429 loss_val: 0.7535 acc_val: 0.8100 time: 0.0196s\n",
            "Epoch: 0158 loss_train: 0.4691 acc_train: 0.9214 loss_val: 0.7523 acc_val: 0.8067 time: 0.0199s\n",
            "Epoch: 0159 loss_train: 0.5199 acc_train: 0.9000 loss_val: 0.7499 acc_val: 0.8067 time: 0.0198s\n",
            "Epoch: 0160 loss_train: 0.4651 acc_train: 0.9286 loss_val: 0.7474 acc_val: 0.8100 time: 0.0202s\n",
            "Epoch: 0161 loss_train: 0.4697 acc_train: 0.8929 loss_val: 0.7449 acc_val: 0.8100 time: 0.0197s\n",
            "Epoch: 0162 loss_train: 0.4685 acc_train: 0.9571 loss_val: 0.7422 acc_val: 0.8167 time: 0.0198s\n",
            "Epoch: 0163 loss_train: 0.4906 acc_train: 0.9143 loss_val: 0.7405 acc_val: 0.8233 time: 0.0202s\n",
            "Epoch: 0164 loss_train: 0.4649 acc_train: 0.9500 loss_val: 0.7391 acc_val: 0.8233 time: 0.0200s\n",
            "Epoch: 0165 loss_train: 0.4761 acc_train: 0.9286 loss_val: 0.7379 acc_val: 0.8167 time: 0.0198s\n",
            "Epoch: 0166 loss_train: 0.4663 acc_train: 0.9071 loss_val: 0.7371 acc_val: 0.8167 time: 0.0234s\n",
            "Epoch: 0167 loss_train: 0.4997 acc_train: 0.9143 loss_val: 0.7351 acc_val: 0.8133 time: 0.0191s\n",
            "Epoch: 0168 loss_train: 0.4707 acc_train: 0.9286 loss_val: 0.7322 acc_val: 0.8133 time: 0.0193s\n",
            "Epoch: 0169 loss_train: 0.4750 acc_train: 0.9286 loss_val: 0.7293 acc_val: 0.8100 time: 0.0193s\n",
            "Epoch: 0170 loss_train: 0.4450 acc_train: 0.9286 loss_val: 0.7272 acc_val: 0.8100 time: 0.0187s\n",
            "Epoch: 0171 loss_train: 0.5026 acc_train: 0.9143 loss_val: 0.7254 acc_val: 0.8133 time: 0.0200s\n",
            "Epoch: 0172 loss_train: 0.4604 acc_train: 0.9071 loss_val: 0.7247 acc_val: 0.8167 time: 0.0198s\n",
            "Epoch: 0173 loss_train: 0.5248 acc_train: 0.9286 loss_val: 0.7242 acc_val: 0.8167 time: 0.0199s\n",
            "Epoch: 0174 loss_train: 0.4496 acc_train: 0.9214 loss_val: 0.7232 acc_val: 0.8133 time: 0.0209s\n",
            "Epoch: 0175 loss_train: 0.4718 acc_train: 0.9000 loss_val: 0.7218 acc_val: 0.8067 time: 0.0208s\n",
            "Epoch: 0176 loss_train: 0.4554 acc_train: 0.9143 loss_val: 0.7216 acc_val: 0.8033 time: 0.0202s\n",
            "Epoch: 0177 loss_train: 0.4627 acc_train: 0.9143 loss_val: 0.7216 acc_val: 0.8067 time: 0.0201s\n",
            "Epoch: 0178 loss_train: 0.4144 acc_train: 0.9357 loss_val: 0.7212 acc_val: 0.8067 time: 0.0227s\n",
            "Epoch: 0179 loss_train: 0.4159 acc_train: 0.9429 loss_val: 0.7190 acc_val: 0.8100 time: 0.0197s\n",
            "Epoch: 0180 loss_train: 0.4126 acc_train: 0.9286 loss_val: 0.7161 acc_val: 0.8100 time: 0.0191s\n",
            "Epoch: 0181 loss_train: 0.4149 acc_train: 0.9429 loss_val: 0.7143 acc_val: 0.8100 time: 0.0234s\n",
            "Epoch: 0182 loss_train: 0.4591 acc_train: 0.9357 loss_val: 0.7123 acc_val: 0.8133 time: 0.0193s\n",
            "Epoch: 0183 loss_train: 0.4185 acc_train: 0.9643 loss_val: 0.7110 acc_val: 0.8200 time: 0.0217s\n",
            "Epoch: 0184 loss_train: 0.4294 acc_train: 0.9286 loss_val: 0.7101 acc_val: 0.8267 time: 0.0226s\n",
            "Epoch: 0185 loss_train: 0.4142 acc_train: 0.9429 loss_val: 0.7101 acc_val: 0.8267 time: 0.0197s\n",
            "Epoch: 0186 loss_train: 0.4377 acc_train: 0.9429 loss_val: 0.7102 acc_val: 0.8200 time: 0.0202s\n",
            "Epoch: 0187 loss_train: 0.4008 acc_train: 0.9571 loss_val: 0.7095 acc_val: 0.8133 time: 0.0197s\n",
            "Epoch: 0188 loss_train: 0.4549 acc_train: 0.9000 loss_val: 0.7102 acc_val: 0.8167 time: 0.0196s\n",
            "Epoch: 0189 loss_train: 0.4130 acc_train: 0.9357 loss_val: 0.7106 acc_val: 0.8167 time: 0.0195s\n",
            "Epoch: 0190 loss_train: 0.4334 acc_train: 0.9214 loss_val: 0.7111 acc_val: 0.8200 time: 0.0195s\n",
            "Epoch: 0191 loss_train: 0.4452 acc_train: 0.9214 loss_val: 0.7114 acc_val: 0.8167 time: 0.0192s\n",
            "Epoch: 0192 loss_train: 0.4478 acc_train: 0.9500 loss_val: 0.7091 acc_val: 0.8167 time: 0.0191s\n",
            "Epoch: 0193 loss_train: 0.3725 acc_train: 0.9571 loss_val: 0.7056 acc_val: 0.8167 time: 0.0198s\n",
            "Epoch: 0194 loss_train: 0.3922 acc_train: 0.9429 loss_val: 0.7036 acc_val: 0.8133 time: 0.0196s\n",
            "Epoch: 0195 loss_train: 0.4569 acc_train: 0.9571 loss_val: 0.7023 acc_val: 0.8133 time: 0.0196s\n",
            "Epoch: 0196 loss_train: 0.3688 acc_train: 0.9714 loss_val: 0.7008 acc_val: 0.8133 time: 0.0236s\n",
            "Epoch: 0197 loss_train: 0.4394 acc_train: 0.9357 loss_val: 0.6997 acc_val: 0.8133 time: 0.0196s\n",
            "Epoch: 0198 loss_train: 0.3985 acc_train: 0.9357 loss_val: 0.6991 acc_val: 0.8133 time: 0.0195s\n",
            "Epoch: 0199 loss_train: 0.4055 acc_train: 0.9286 loss_val: 0.6997 acc_val: 0.8133 time: 0.0199s\n",
            "Epoch: 0200 loss_train: 0.4038 acc_train: 0.9429 loss_val: 0.7002 acc_val: 0.8133 time: 0.0197s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.1684s\n",
            "Test set results: loss= 0.7330 accuracy= 0.8350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZCnAqNHagkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33db101f-0d7f-454e-e578-06ef3eb0c21f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  figure.png  LICENCE  pygcn  README.md  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdrtNOHjaj5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}